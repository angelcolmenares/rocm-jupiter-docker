{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989267e7-c68a-4e20-9358-e7c23fc20171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 68GB VRAM Comprehensive Benchmark ===\n",
      "PyTorch version: 2.9.1+rocm7.1.1.git351ff442\n",
      "ROCm available: True\n",
      "Total VRAM: 68.72 GB\n",
      "Device: AMD Radeon Graphics\n",
      "ROCm version: 7.1.52802-26aae437f6\n",
      "\n",
      "==================================================\n",
      "TEST 1: Large Tensor Operations\n",
      "==================================================\n",
      "Allocating 100000x100000 matrix in float16...\n",
      "✓ Allocated 20.00 GB in 0.07 seconds\n",
      "Performing matrix multiplication (subset)...\n",
      "✓ Matrix multiplication: 0.16 seconds\n",
      "\n",
      "==================================================\n",
      "TEST 2: Large Model Training\n",
      "==================================================\n",
      "Creating large neural network...\n",
      "Model parameters: 313,166,090\n",
      "Model size: 0.63 GB (float16)\n",
      "Creating synthetic data (batch_size=64)...\n",
      "Running training steps...\n",
      "  Step 0: loss=2.3008, time=0.199s\n",
      "  Step 2: loss=nan, time=0.070s\n",
      "  Step 4: loss=nan, time=0.069s\n",
      "  Step 6: loss=nan, time=0.069s\n",
      "  Step 8: loss=nan, time=0.069s\n",
      "✓ Average step time: 0.082 seconds\n",
      "\n",
      "==================================================\n",
      "TEST 3: Memory Management & Fragmentation\n",
      "==================================================\n",
      "Testing memory allocation pattern...\n",
      "  Allocating 2GB tensor (22360x22360)...\n",
      "  ✓ Success - Total allocated: 3.41 GB\n",
      "  Allocating 4GB tensor (31622x31622)...\n",
      "  ✓ Success - Total allocated: 7.41 GB\n",
      "  Allocating 8GB tensor (44721x44721)...\n",
      "  ✓ Success - Total allocated: 15.41 GB\n",
      "  Allocating 16GB tensor (63245x63245)...\n",
      "  ✓ Success - Total allocated: 31.42 GB\n",
      "Cleaning up...\n",
      "\n",
      "==================================================\n",
      "TEST 4: Multi-Task Parallelism\n",
      "==================================================\n",
      "Testing parallel tensor operations...\n",
      "Created 10 tensors in 0.00 seconds\n",
      "Total memory: 5.42 GB\n",
      "  Processed tensor 1/10\n",
      "  Processed tensor 3/10\n",
      "  Processed tensor 5/10\n",
      "  Processed tensor 7/10\n",
      "  Processed tensor 9/10\n",
      "Processed all tensors in 0.05 seconds\n",
      "\n",
      "==================================================\n",
      "FINAL MEMORY REPORT\n",
      "==================================================\n",
      "Currently allocated: 1.81 GB\n",
      "Currently reserved:  1.88 GB\n",
      "Max allocated:       35.42 GB\n",
      "Available VRAM:      66.91 GB\n",
      "\n",
      "Memory breakdown:\n",
      "  GPU 0: AMD Radeon Graphics\n",
      "    Total: 68.72 GB\n",
      "    Multiprocessors: 20\n",
      "\n",
      "==================================================\n",
      "MEMORY MANAGEMENT TIPS\n",
      "==================================================\n",
      "1. Use float16 for large models (half memory)\n",
      "2. Clear cache regularly: torch.cuda.empty_cache()\n",
      "3. Use gradient checkpointing for huge models\n",
      "4. Set environment variable to reduce fragmentation:\n",
      "   PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
      "5. Use batch sizes that fit in memory\n",
      "6. Your system can handle models up to ~55 GB\n",
      "\n",
      "==================================================\n",
      "SYSTEM INFORMATION\n",
      "==================================================\n",
      "CPU: 32 cores\n",
      "RAM: 65.70 GB total\n",
      "Disk: 491.00 GB total\n",
      "Python: 3.12.3 (main, Jan  8 2026, 11:30:50) [GCC 13.3.0]\n",
      "\n",
      "==================================================\n",
      "BENCHMARK COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# workspace/notebooks/gpu/benchmark_68gb_vram.ipynb\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=== 68GB VRAM Comprehensive Benchmark ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ROCm available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    total_memory_gb = torch.cuda.get_device_properties(device).total_memory / 1e9\n",
    "    print(f\"Total VRAM: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ROCm version: {torch.version.hip}\")\n",
    "    \n",
    "    # Clear any previous allocations\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # ===== TEST 1: Large Tensor Operations =====\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST 1: Large Tensor Operations\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test with half precision (float16) to save memory\n",
    "    dtype = torch.float16\n",
    "    \n",
    "    # Try allocating 20GB tensor\n",
    "    try:\n",
    "        # 20GB in float16: 20e9 bytes / 2 bytes per element = 10e9 elements\n",
    "        # Let's do sqrt(10e9) ≈ 100,000 for square matrix\n",
    "        size = 100000\n",
    "        print(f\"Allocating {size}x{size} matrix in float16...\")\n",
    "        \n",
    "        start = time.time()\n",
    "        A = torch.randn(size, size, dtype=dtype, device=device)\n",
    "        allocation_time = time.time() - start\n",
    "        \n",
    "        allocated_gb = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"✓ Allocated {allocated_gb:.2f} GB in {allocation_time:.2f} seconds\")\n",
    "        \n",
    "        # Matrix multiplication (smaller to avoid OOM)\n",
    "        print(\"Performing matrix multiplication (subset)...\")\n",
    "        start = time.time()\n",
    "        B = A[:1000, :1000]  # Take subset\n",
    "        C = B @ B.T\n",
    "        mm_time = time.time() - start\n",
    "        print(f\"✓ Matrix multiplication: {mm_time:.2f} seconds\")\n",
    "        \n",
    "        del A, B, C\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "    \n",
    "    # ===== TEST 2: Large Model Training =====\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST 2: Large Model Training\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Create a large model (~10GB parameters in float16)\n",
    "        print(\"Creating large neural network...\")\n",
    "        model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8192, 16384),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16384, 8192),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8192, 4096),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4096, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 10)\n",
    "        ).to(device).to(dtype)  # Convert to half precision\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Model parameters: {total_params:,}\")\n",
    "        print(f\"Model size: {total_params * 2 / 1e9:.2f} GB (float16)\")\n",
    "        \n",
    "        # Training setup\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Create synthetic data\n",
    "        batch_size = 64\n",
    "        print(f\"Creating synthetic data (batch_size={batch_size})...\")\n",
    "        inputs = torch.randn(batch_size, 8192, dtype=dtype, device=device)\n",
    "        targets = torch.randint(0, 10, (batch_size,), device=device)\n",
    "        \n",
    "        # Training loop\n",
    "        print(\"Running training steps...\")\n",
    "        times = []\n",
    "        for step in range(10):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            step_time = time.time() - start\n",
    "            times.append(step_time)\n",
    "            \n",
    "            if step % 2 == 0:\n",
    "                print(f\"  Step {step}: loss={loss.item():.4f}, time={step_time:.3f}s\")\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        print(f\"✓ Average step time: {avg_time:.3f} seconds\")\n",
    "        \n",
    "        del model, optimizer, inputs, targets\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "    \n",
    "    # ===== TEST 3: Memory Management =====\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST 3: Memory Management & Fragmentation\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Try allocating multiple large tensors to test fragmentation\n",
    "        print(\"Testing memory allocation pattern...\")\n",
    "        \n",
    "        tensor_sizes = [2, 4, 8, 16]  # GB\n",
    "        tensors = []\n",
    "        \n",
    "        for i, size_gb in enumerate(tensor_sizes):\n",
    "            try:\n",
    "                # Calculate elements needed for size_gb in float32\n",
    "                elements = int(size_gb * 1e9 / 4)\n",
    "                dim = int(np.sqrt(elements))\n",
    "                \n",
    "                print(f\"  Allocating {size_gb}GB tensor ({dim}x{dim})...\")\n",
    "                tensor = torch.randn(dim, dim, dtype=torch.float32, device=device)\n",
    "                tensors.append(tensor)\n",
    "                print(f\"  ✓ Success - Total allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Failed at {size_gb}GB: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Clean up\n",
    "        print(\"Cleaning up...\")\n",
    "        for tensor in tensors:\n",
    "            del tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "    \n",
    "    # ===== TEST 4: Multi-Task Parallelism =====\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST 4: Multi-Task Parallelism\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        print(\"Testing parallel tensor operations...\")\n",
    "        \n",
    "        # Create multiple smaller tensors\n",
    "        num_tensors = 10\n",
    "        tensor_size = (1000, 1000, 100)  # ~400MB each in float32\n",
    "        \n",
    "        start = time.time()\n",
    "        tensors = [torch.randn(*tensor_size, device=device) for _ in range(num_tensors)]\n",
    "        creation_time = time.time() - start\n",
    "        \n",
    "        print(f\"Created {num_tensors} tensors in {creation_time:.2f} seconds\")\n",
    "        print(f\"Total memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Process them\n",
    "        start = time.time()\n",
    "        results = []\n",
    "        for i, tensor in enumerate(tensors):\n",
    "            result = tensor.mean() + tensor.std()\n",
    "            results.append(result)\n",
    "            if i % 2 == 0:\n",
    "                print(f\"  Processed tensor {i+1}/{num_tensors}\")\n",
    "        \n",
    "        process_time = time.time() - start\n",
    "        print(f\"Processed all tensors in {process_time:.2f} seconds\")\n",
    "        \n",
    "        del tensors, results\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "    \n",
    "    # ===== FINAL MEMORY REPORT =====\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL MEMORY REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    max_allocated = torch.cuda.max_memory_allocated() / 1e9\n",
    "    \n",
    "    print(f\"Currently allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Currently reserved:  {reserved:.2f} GB\")\n",
    "    print(f\"Max allocated:       {max_allocated:.2f} GB\")\n",
    "    print(f\"Available VRAM:      {total_memory_gb - allocated:.2f} GB\")\n",
    "    \n",
    "    # Memory info\n",
    "    print(\"\\nMemory breakdown:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Total: {props.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"    Multiprocessors: {props.multi_processor_count}\")\n",
    "    \n",
    "    # Memory management tips\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MEMORY MANAGEMENT TIPS\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. Use float16 for large models (half memory)\")\n",
    "    print(\"2. Clear cache regularly: torch.cuda.empty_cache()\")\n",
    "    print(\"3. Use gradient checkpointing for huge models\")\n",
    "    print(\"4. Set environment variable to reduce fragmentation:\")\n",
    "    print(\"   PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")\n",
    "    print(\"5. Use batch sizes that fit in memory\")\n",
    "    print(f\"6. Your system can handle models up to ~{total_memory_gb * 0.8:.0f} GB\")\n",
    "    \n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "\n",
    "# System info\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"CPU: {psutil.cpu_count()} cores\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / 1e9:.2f} GB total\")\n",
    "print(f\"Disk: {psutil.disk_usage('/').total / 1e9:.2f} GB total\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BENCHMARK COMPLETE!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a0108-45c7-4592-8553-588a6f158e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
